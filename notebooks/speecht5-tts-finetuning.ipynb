{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers datasets soundfile speechbrain accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:35.519662Z","iopub.execute_input":"2025-04-01T16:51:35.519902Z","iopub.status.idle":"2025-04-01T16:51:42.051617Z","shell.execute_reply.started":"2025-04-01T16:51:35.519877Z","shell.execute_reply":"2025-04-01T16:51:42.050708Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:42.052481Z","iopub.execute_input":"2025-04-01T16:51:42.052727Z","iopub.status.idle":"2025-04-01T16:51:42.465656Z","shell.execute_reply.started":"2025-04-01T16:51:42.052705Z","shell.execute_reply":"2025-04-01T16:51:42.464823Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fcc1dbf57d0478b948769ec231785b7"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Dataset\n","metadata":{}},{"cell_type":"code","source":"import datasets\nfrom datasets import load_dataset\n\ndataset_1 = load_dataset('Cets/audio-logs-vn-1')\ndataset_2 = load_dataset('toandev/ThanhPahm-TTS')\ndataset_3 = load_dataset('doof-ferb/vais1000')\ndataset_4 = load_dataset('doof-ferb/infore1_25hours')\nprint(dataset_1['train'].features)\nprint(dataset_2)\nprint(dataset_3)\nprint(dataset_4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:53:23.783174Z","iopub.execute_input":"2025-04-01T16:53:23.783536Z","iopub.status.idle":"2025-04-01T16:55:00.818774Z","shell.execute_reply.started":"2025-04-01T16:53:23.783508Z","shell.execute_reply":"2025-04-01T16:55:00.817916Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/255M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689f899a0dfa4156af8cbb4e02f773b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"513381af004c47b588fe28b16c39fdca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59133f3438c4477abbea9a3a9aa72dc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0d767c82294f4caa143bc287f30160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb06a9230c0430aab192dea60f0cda7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e31ea5f74894d238207f3c345e54048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fd420c18b34a9e8d92ff3c99572df8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00015.parquet:   0%|          | 0.00/486M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd5f8ffd867542a78af0a98c5b0cc70f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00015.parquet:   0%|          | 0.00/485M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"234b97920c314b82884a2952fe718cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00015.parquet:   0%|          | 0.00/522M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31c8b4719d50410baf4f3c255f4b9b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00015.parquet:   0%|          | 0.00/506M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3be6dfdb1a04850828047bf8ace0350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00015.parquet:   0%|          | 0.00/495M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f173779ca1c94a2a888dcff0f6a3e0ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00015.parquet:   0%|          | 0.00/537M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d645460973064989861eccefb9903f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00015.parquet:   0%|          | 0.00/538M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69fba0ebb8b94612883cc3b989b017ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00015.parquet:   0%|          | 0.00/514M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b82048efdb14900b2e68068c60fe3a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00015.parquet:   0%|          | 0.00/507M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6e656ddf9442119709cabcea4a7f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00015.parquet:   0%|          | 0.00/534M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a84bf7b7355411a9c5b7a13134ec3ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00010-of-00015.parquet:   0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f0c8d6e4cf418c843ada00ef10dc4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00011-of-00015.parquet:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c796da74c07e4c85acfb6be23810b2d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00012-of-00015.parquet:   0%|          | 0.00/551M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfbf73cc83e94af6b85523e6a35a79d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00013-of-00015.parquet:   0%|          | 0.00/555M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b03921e1c7f747a59cd74342e332e60f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00014-of-00015.parquet:   0%|          | 0.00/537M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0a3dd91a474b58b034db8a16d4d9ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14935 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a655fd40dd8e489dbd61ca1c0914aafb"}},"metadata":{}},{"name":"stdout","text":"{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None)}\nDatasetDict({\n    train: Dataset({\n        features: ['audio', 'transcription'],\n        num_rows: 4000\n    })\n})\nDatasetDict({\n    train: Dataset({\n        features: ['audio', 'transcription'],\n        num_rows: 1000\n    })\n})\nDatasetDict({\n    train: Dataset({\n        features: ['audio', 'transcription'],\n        num_rows: 14935\n    })\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"dataset_2 = dataset_2.rename_column('transcription', 'text')\ndataset_2 = dataset_2.cast_column('audio', datasets.Audio(sampling_rate=16000))\ndataset_3 = dataset_3.rename_column('transcription', 'text')\ndataset_3 = dataset_3.cast_column('audio', datasets.Audio(sampling_rate=16000))\ndataset_4 = dataset_4.rename_column('transcription', 'text')\ndataset_4 = dataset_4.cast_column('audio', datasets.Audio(sampling_rate=16000))\nprint(dataset_2['train'].features)\nprint(dataset_3['train'].features)\nprint(dataset_4['train'].features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:55:00.819961Z","iopub.execute_input":"2025-04-01T16:55:00.820268Z","iopub.status.idle":"2025-04-01T16:55:00.845490Z","shell.execute_reply.started":"2025-04-01T16:55:00.820235Z","shell.execute_reply":"2025-04-01T16:55:00.844762Z"}},"outputs":[{"name":"stdout","text":"{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'text': Value(dtype='string', id=None)}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"dataset_1 = dataset_1.map(lambda x: {'text': x['text'].lower(),'speaker_id':1})\ndataset_2 = dataset_2.map(lambda x: {'text': x['text'].lower(),'speaker_id':2})\ndataset_3 = dataset_3.map(lambda x: {'text': x['text'].lower(),'speaker_id':3})\ndataset_4 = dataset_4.map(lambda x: {'text': x['text'].lower(),'speaker_id':4})\n\n\ndataset_2 = dataset_2['train'].train_test_split(test_size=0.2, shuffle=True)\ndataset_3 = dataset_3['train'].train_test_split(test_size=0.2, shuffle=True)\ndataset_4 = dataset_4['train'].train_test_split(test_size=0.2, shuffle=True)\n\nprint(dataset_1)\nprint(dataset_2)\nprint(dataset_3)\nprint(dataset_4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:55:00.847097Z","iopub.execute_input":"2025-04-01T16:55:00.847310Z","execution_failed":"2025-04-01T16:55:31.737Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1913 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8875438ccac44f5295e2e5a82903e432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/479 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9407c81fe92e4292b9bb9a005fc0fd85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380f3f9d67824b39a9df87aa618bd977"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d460af8ef8e4a3eb17e2332ac049963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14935 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466c7443ccb744799eced4ec0490f7c9"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from datasets import concatenate_datasets, DatasetDict\n\nmerged_train = concatenate_datasets([dataset_1['train'], dataset_2['train'], dataset_3['train'], dataset_4['train']])\nmerged_test = concatenate_datasets([dataset_1['test'], dataset_2['test'], dataset_3['test'], dataset_4['test']])\n\ndataset = DatasetDict({'train': merged_train, 'test':merged_test})\nprint(dataset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Special characters","metadata":{}},{"cell_type":"code","source":"from transformers import SpeechT5ForTextToSpeech, SpeechT5Processor\n\ncheckpoint = \"microsoft/speecht5_tts\"\nprocessor = SpeechT5Processor.from_pretrained(checkpoint)\ntokenizer = processor.tokenizer","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_all_chars(batch):\n    all_text = \" \".join(batch[\"text\"])\n    vocab = list(set(all_text))\n    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n\n\nvocabs = dataset.map(\n    extract_all_chars,\n    batched=True,\n    batch_size=-1,\n    keep_in_memory=True,\n    remove_columns=dataset.column_names['train'],\n)\n\ndataset_vocab = set(vocabs['train'][\"vocab\"][0] + vocabs['test']['vocab'][0])\ntokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\nnew_tokens = list(dataset_vocab - tokenizer_vocab)\nprint(new_tokens)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n\ntokenizer.add_tokens(new_tokens)\nmodel.resize_token_embeddings(len(tokenizer))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Speaker embedding\n\n*! We use a X-vector model for english now, it is possible to train a new X-vector model*","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom speechbrain.pretrained import EncoderClassifier\n\nspk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nspeaker_model = EncoderClassifier.from_hparams(\n    source=spk_model_name,\n    run_opts={\"device\": device},\n    savedir=os.path.join(\"/tmp\", spk_model_name),\n)\n\n\ndef create_speaker_embeddings(waveform):\n    with torch.no_grad():\n        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n    return speaker_embeddings","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef calculate_speaker_embeddings(dataset):\n    embeddings = []\n    for split in ['train', 'test']:\n        for sample in dataset[split]:\n            audio = sample['audio']['array']\n            embedding = create_speaker_embeddings(audio)\n            embeddings.append(embedding)\n    speaker_embeddings = np.mean(np.array(embeddings), axis=0)\n    print(speaker_embeddings.shape)\n    return speaker_embeddings\n\nspeaker_embeddings_1 = calculate_speaker_embeddings(dataset_1)\nspeaker_embeddings_2 = calculate_speaker_embeddings(dataset_2)\nspeaker_embeddings_3 = calculate_speaker_embeddings(dataset_3)\nspeaker_embeddings_4 = calculate_speaker_embeddings(dataset_4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef prepare_dataset(example, speaker_embeddings_1=speaker_embeddings_1, speaker_embeddings_2=speaker_embeddings_2):\n    audio = example[\"audio\"]\n    speaker_id = example['speaker_id']\n    \n    example = processor(\n        text=example[\"text\"],\n        audio_target=audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"],\n        return_attention_mask=False,\n    )\n\n    # strip off the batch dimension\n    example[\"labels\"] = example[\"labels\"][0]\n\n    # use SpeechBrain to obtain x-vector (here we used a speaker_embedding for all sample because we have 1 speaker)\n    if speaker_id == 1:\n        example[\"speaker_embeddings\"] = speaker_embeddings_1\n    elif speaker_id == 2:\n        example[\"speaker_embeddings\"] = speaker_embeddings_2\n    elif speaker_id == 3:\n        example[\"speaker_embeddings\"] = speaker_embeddings_3\n    elif speaker_id == 4:\n        example[\"speaker_embeddings\"] = speaker_embeddings_4\n        \n    return example\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processed_example = prepare_dataset(dataset['train'][0])\nprint(list(processed_example.keys()))\nprint(processed_example[\"speaker_embeddings\"].shape)\nimport matplotlib.pyplot as plt\n\nplt.figure()\nplt.imshow(processed_example[\"labels\"].T)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names['train'])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data collator","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n\n@dataclass\nclass TTSDataCollatorWithPadding:\n    processor: Any\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n    ) -> Dict[str, torch.Tensor]:\n        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n\n        # collate the inputs and targets into a batch\n        batch = processor.pad(\n            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n        )\n\n        # replace padding with -100 to ignore loss correctly\n        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n        )\n\n        # not used during fine-tuning\n        del batch[\"decoder_attention_mask\"]\n\n        # round down target lengths to multiple of reduction factor\n        if model.config.reduction_factor > 1:\n            target_lengths = torch.tensor(\n                [len(feature[\"input_values\"]) for feature in label_features]\n            )\n            target_lengths = target_lengths.new(\n                [\n                    length - length % model.config.reduction_factor\n                    for length in target_lengths\n                ]\n            )\n            max_length = max(target_lengths)\n            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n\n        # also add in the speaker embeddings\n        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n\n        return batch\n\ndata_collator = TTSDataCollatorWithPadding(processor=processor)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from functools import partial\n# disable cache during training since it's incompatible with gradient checkpointing\nmodel.config.use_cache = False\n\n# set language and task for generation and re-enable cache\nmodel.generate = partial(model.generate, use_cache=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"speechT5-vn-1\",  # change to a repo name of your choice\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-3,\n    warmup_steps=50,\n    max_steps=4000,\n    gradient_checkpointing=True,\n    fp16=True,\n    eval_strategy=\"steps\",\n    per_device_eval_batch_size=2,\n    save_steps=1000,\n    eval_steps=100,\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    greater_is_better=False,\n    label_names=[\"labels\"],\n    push_to_hub=True,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    processing_class=processor,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T16:55:31.741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:51:55.206450Z","iopub.status.idle":"2025-04-01T16:51:55.206684Z","shell.execute_reply":"2025-04-01T16:51:55.206589Z"}},"outputs":[],"execution_count":null}]}